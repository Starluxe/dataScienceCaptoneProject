---
title: "Data Science Captone Project: Milestone Report"
author: "Marcela Castro Le√≥n"
date: "May 1, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, cache.lazy = FALSE)
```

## Introduction 
This work corresponds to the Peer-graded Assignment: Milestone Report. 
The aim is to show the exploration data and text mining done so far to prepare the final project of the data science captone project. 
This work have three parts. First, the text data comming from blogs, news and twitter in english  is read and 10% is selected. Secondly, we create the corpus, clean the data and analyze the most frequence words and the combination of two and three words (n-grams). Finally we make some plots to show the result of the exploratory analysis. Moreover, in the end, the approach for the next step in the project is stated. 


## Reading and sampling input data
In this section, the file are readed and a sample is selected and save to avoid reading large files all the executions. 

```{r, echo=FALSE, warning=FALSE}
suppressMessages(library(ggplot2))
suppressMessages(library(NLP))
library(tm)
library(SnowballC)
library(stringi)
library(RColorBrewer)
library(wordcloud)
library(RWeka)
library(slam)
```
The original files are data comming from blogs, news and twitter in english language. The files are quite large, 8.9, 10.1 and 23.6 millons of records for blogs, news and twitter respectively. All of them have more than 30 millons of words. In this part we read the files and select randomly 10% of the lines.

```{r, warning=FALSE, echo=FALSE}
readSampleFile<-function(fileName) {
  paste("Reading 5% sample of file ",fileName)
  con=file(fileName,open="r")
  qtotlines<-0     # number of total lines
  qselectedlines <-0   #number of total selected lines
  qtotchar<-0     # number of total chars in file
  qcharselectedlines <-0   #number of total char in selected lines
  finaldata<-NULL
  block=5000
  longmaxline=0;
  while ( TRUE ) {
      line = readLines(con, n = block,skipNul=TRUE)
      if ( length(line) == 0 ) {
          break
      }
      if(sample(1:20,1)==5) {  #random 1:20==5 to select 5% of the lines
          qselectedlines<-qselectedlines+block
          qcharselectedlines<-qcharselectedlines+sum(lengths(strsplit(line, "\\W+")))
          finaldata<-c(finaldata,line)
      }
      lenline=max(nchar(line))
      if(lenline > longmaxline) {
        longmaxline=lenline
      }
      qtotlines <- qtotlines+block
      qtotchar<-qtotchar+sum(lengths(strsplit(line, "\\W+")))
  }
cat(sprintf("File: %s Lines: %.2fM Selected: %.2fM Chars: %.2fM  Selected: %.2fM MaxLine %d\n ", basename(fileName),qtotlines/1000000,qselectedlines/1000000,qtotchar/1000000,qcharselectedlines/1000000,longmaxline))
close(con)
return(finaldata)
}

input_sample_file="final_data.txt"
if(!file.exists(input_sample_file)){
    us_blogs <- readSampleFile("final/en_US/en_US.blogs.txt")
    us_news  <- readSampleFile("final/en_US/en_US.news.txt")
    us_twitter  <- readSampleFile("final/en_US/en_US.twitter.txt")
    final_data=c(us_blogs,us_news, us_twitter)
    writeLines(final_data, "final_data.txt")
    } else {
  final_data<-readLines(input_sample_file, encoding = "UTF-8", skipNul=TRUE)
    }
print("General stats for sample final data")
stri_stats_general(final_data)
```

## Exploring and cleaning final input data 
Using tm package, we create the Corpus object and clean the sample data by 

- converting to lower case
- remove puntuation  
- remove numbers
- remove extra whitespaces 
- remove common english words
- remove profanity words

A sample of 5K lines are choosed in order to be able to generate the matrix.

```{r, echo=TRUE, warning=FALSE}
set.seed(1234)
final_data_sample = sample(final_data,5000, replace = FALSE)
#creating corpus object to use tn functions
final_data_cleaned <- VCorpus(VectorSource(final_data_sample))
#cleaning operations on sample text: lower, remove punctuation, remove common words, etc
final_data_cleaned <- tm_map(final_data_cleaned, content_transformer(tolower))
final_data_cleaned <- tm_map(final_data_cleaned, content_transformer(removePunctuation))
final_data_cleaned <- tm_map(final_data_cleaned, stripWhitespace)
final_data_cleaned <- tm_map(final_data_cleaned, removeWords, stopwords("english"))
#final_data_cleaned <- tm_map(final_data_cleaned, removeWords, unlist(profanityWords))
final_data_cleaned <- tm_map(final_data_cleaned, removeNumbers)
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
final_data_cleaned <- tm_map(final_data_cleaned,  content_transformer(removeURL))
#remove profanity words
profanityWords <- read.table("./full-list-of-bad-words-text-file_2018_03_26.txt", header = FALSE)
final_data_cleaned <- tm_map(final_data_cleaned, removeWords, unlist(profanityWords))
```

We create a document term matrix to calculate the frequency for one, two and three words. 

```{r, echo=TRUE, warning=FALSE}
#functions to create tokens of two and three words using Weka package
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
#creating matrix of terms for one,two and three words and frequency vectors
#one word (1-grams)
dtm<-DocumentTermMatrix(final_data_cleaned)
dtm_bigram <- DocumentTermMatrix(final_data_cleaned, control = list(tokenize = BigramTokenizer, stemming = TRUE))
dtm_trigram <- DocumentTermMatrix(final_data_cleaned, control = list(tokenize = TrigramTokenizer, stemming = TRUE))
dtmmat_unigram<-as.matrix(dtm)
freq<-colSums(dtmmat_unigram)
freq<-sort(freq,decreasing = TRUE)
#two words (2-grams)
dtmmat_bigram<-as.matrix(dtm_bigram)
freq_bigram<-colSums(dtmmat_bigram)
freq_bigram<-sort(freq_bigram,decreasing = TRUE)
#three words (3-grams)
dtmmat_trigram<-as.matrix(dtm_trigram)
freq_trigram<-colSums(dtmmat_trigram)
freq_trigram<-sort(freq_trigram,decreasing = TRUE)
```

A wordcloud and histogram are plotted for every n-gram.

```{r, echo=TRUE, warning=FALSE}
words<-names(freq)
wordcloud(words[1:100],freq[1:100],random.order = F,random.color = F,          colors = brewer.pal(9,"Blues"))

words_bigram<-names(freq_bigram)
wordcloud(words_bigram[1:100],freq_bigram[1:100],random.order = F,random.color = F,          colors = brewer.pal(9,"Blues"))

words_trigram<-names(freq_trigram)
wordcloud(words_trigram[1:100],freq_trigram[1:100],random.order = F,random.color = F,colors = brewer.pal(9,"Blues"))

unigram_freq <- rowapply_simple_triplet_matrix(dtm,sum)
bigram_freq <- rowapply_simple_triplet_matrix(dtm_bigram,sum)
trigram_freq <- rowapply_simple_triplet_matrix(dtm_trigram,sum)
par(mfrow = c(1,3), oma=c(0,0,3,0))
hist(unigram_freq, breaks = 50, main = 'unigram freq', xlab='frequency')
hist(bigram_freq, breaks = 50, main = 'bigram freq', xlab='frequency')
hist(trigram_freq, breaks = 50, main = 'trigram freq', xlab='frequency')
title("NGram Histograms",outer=T)
```


The histograms shows that the distribution is skewed.  We analize how many most frequency words we need to cover the 50% of total words. 

```{r, echo=TRUE, warning=FALSE}
#Words in freq
length(freq)
#sum of frequencies
freqtot<-sum(freq)
#sum freq of first 200 words
freq150<-sum(freq[1:150])
calcfreq <- function(freq, i) {
    freqtot<-sum(freq)
    freqtot
    freqi<-sum(freq[1:i])
    ratio<-i/length(freq)
    coverage<-freqi/freqtot
    cat(sprintf("Tot words: %d Analized (top frequency) %d words Ratio=%.2f Coverage %.2f\n",length(freq),i,ratio,coverage))
}    
for (i in seq(100,1000,100)) {
  calcfreq(freq,i) 
}
```

## Ploting the results

Finally, we plot the 30 most frequent one, two and three words in the sample.

```{r, echo=TRUE, warning=FALSE}
num <- 30
unigram_df <- head(data.frame(terms=names(freq), freq=freq),n=num) 
bigram_df <- head(data.frame(terms=names(freq_bigram), freq=freq_bigram),n=num)
trigram_df <- head(data.frame(terms=names(freq_trigram), freq=freq_trigram),n=num)

#Plot 1 - Unigram
plot_unigram  <-ggplot(unigram_df,aes(terms, freq))   
plot_unigram  <- plot_unigram  + geom_bar(fill="white",colour=unigram_df$freq,stat="identity") + scale_x_discrete(limits=unigram_df$terms)
plot_unigram  <- plot_unigram  + theme(axis.text.x=element_text(angle=45, hjust=1))  
plot_unigram  <- plot_unigram  + labs(x = "Words", y="Frequency", title="30 most frequent words in Sample")
plot_unigram  

plot_bigram <-ggplot(bigram_df,aes(terms, freq))   
plot_bigram <-plot_bigram + geom_bar(fill="white", colour=bigram_df$freq,stat="identity") + scale_x_discrete(limits=bigram_df$terms)
plot_bigram <- plot_bigram + theme(axis.text.x=element_text(angle=45, hjust=1))  
plot_bigram<- plot_bigram+ labs(x = "Words", y="Frequency", title="30 most frequent two-words in Sample")
plot_bigram

plot_trigram <-ggplot(trigram_df,aes(terms, freq))   
plot_trigram <- plot_trigram + geom_bar(fill="white",colour=trigram_df$freq,stat="identity") + scale_x_discrete(limits=trigram_df$terms)
plot_trigram <- plot_trigram + theme(axis.text.x=element_text(angle=45, hjust=1))  
plot_trigram <- plot_trigram + labs(x = "Words", y="Frequency", title="Top 30 Trigrams in Sample")
plot_trigram
```


## Conclusions and next steps

The next step is to build the predictive algorithm to know the next word which are going to be write in a sentence. The data to be used for training and testing the algorithm is the n-grams we have been building in this projects.
The algorithm have to be optimized in order to be executed with low memory and cpu, so the resources have to be measured.

